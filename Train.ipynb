{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of this notebook is to showcase a cheap, fast manner to handle  text classification tasks without any fancy hardware. \n",
    "\n",
    "#### PS: I am using my Macbook Air. \n",
    "\n",
    "#### Step 1: \n",
    "    \n",
    "    Transform the text using TF-IDF feature extracter by using character n_gram range betwen 1 and 2.\n",
    "    \n",
    "#### Step 2: \n",
    "    \n",
    "    Reduce the TF-IDF vectors using Truncated SVD by capturing the maximum level of variance. The go to method for    sparse matrices instead of PCA. \n",
    "    \n",
    "#### Step 3: \n",
    "    \n",
    "    Run a logistic regression Model on the newly truncated vectors. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_lang.csv\")\n",
    "valid = pd.read_csv(\"valid_lang.csv\")\n",
    "test = pd.read_csv(\"test_lang.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TF-IDF feature vectors. \n",
    "\n",
    "Building features on top of characters instead of words makes more sense for language detection since some languages have intrinsic characters that others don't, also we can use bi-grams while still keeping the dimensionality low meaning not in the 100,000. Hence my macbook air will not run into memory errors when running the svd algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bouyida/anaconda3/envs/analytics/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), analyzer='char')\n",
    "train_tfidf = tfidf.fit_transform(train[\"text\"].values)\n",
    "valid_tfidf = tfidf.transform(valid[\"text\"].values)\n",
    "test_tfidf = tfidf.transform(test[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84000, 6764)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf.shape # check the dimensionality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dimensionality to 400 featurs. \n",
    "The n_components parameter was tuned to ensure a good performance on the Logistic regression. Using the arpack prevented my laptop from running into memory error. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 400, algorithm = \"arpack\" )\n",
    "train_svd = svd.fit_transform(train_tfidf)\n",
    "valid_svd = svd.transform(valid_tfidf)\n",
    "test_svd = svd.transform(test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how much variance are we capturing the truncated features are capturing. Usually a ratio of 0.9 is recommeneded\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.781500797964849"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.explained_variance_ratio_.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic regression model and get the validation results. \n",
    "\n",
    "Since this is a multiclass classification problem, precision, recall and F1 score is are the right evaluation metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         bg       1.00      1.00      1.00      1000\n",
      "         cs       0.97      0.96      0.97      1000\n",
      "         da       0.98      0.98      0.98      1000\n",
      "         de       0.98      0.98      0.98      1000\n",
      "         el       1.00      0.98      0.99      1000\n",
      "         en       0.95      0.99      0.97      1000\n",
      "         es       0.98      0.98      0.98      1000\n",
      "         et       0.98      0.98      0.98      1000\n",
      "         fi       0.99      0.99      0.99      1000\n",
      "         fr       0.98      0.98      0.98      1000\n",
      "         hu       0.99      0.99      0.99      1000\n",
      "         it       0.97      0.99      0.98      1000\n",
      "         lt       0.98      0.98      0.98      1000\n",
      "         lv       1.00      0.98      0.99      1000\n",
      "         nl       0.98      0.97      0.98      1000\n",
      "         pl       1.00      0.99      0.99      1000\n",
      "         pt       0.98      0.98      0.98      1000\n",
      "         ro       0.99      0.99      0.99      1000\n",
      "         sk       0.97      0.95      0.96      1000\n",
      "         sl       0.98      0.99      0.98      1000\n",
      "         sv       0.99      0.97      0.98      1000\n",
      "\n",
      "avg / total       0.98      0.98      0.98     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_svd, train[\"lang\"])\n",
    "valid_preds = lr.predict(valid_svd)\n",
    "print(classification_report(valid[\"lang\"], valid_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         bg       1.00      1.00      1.00      1000\n",
      "         cs       0.99      0.99      0.99      1000\n",
      "         da       0.99      0.99      0.99      1000\n",
      "         de       0.99      1.00      0.99      1000\n",
      "         el       1.00      1.00      1.00      1000\n",
      "         en       0.99      1.00      1.00      1000\n",
      "         es       0.99      0.99      0.99      1000\n",
      "         et       1.00      0.99      0.99      1000\n",
      "         fi       1.00      1.00      1.00      1000\n",
      "         fr       1.00      0.99      0.99      1000\n",
      "         hu       1.00      1.00      1.00      1000\n",
      "         it       0.99      1.00      0.99      1000\n",
      "         lt       1.00      1.00      1.00      1000\n",
      "         lv       1.00      1.00      1.00      1000\n",
      "         nl       0.99      0.99      0.99      1000\n",
      "         pl       1.00      1.00      1.00      1000\n",
      "         pt       0.99      0.99      0.99      1000\n",
      "         ro       1.00      1.00      1.00      1000\n",
      "         sk       0.99      0.99      0.99      1000\n",
      "         sl       1.00      0.99      1.00      1000\n",
      "         sv       0.99      0.99      0.99      1000\n",
      "\n",
      "avg / total       0.99      0.99      0.99     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test[\"lang\"], lr.predict(test_svd)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
